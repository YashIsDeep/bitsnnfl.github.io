---
layout: post
share: true
title: 'StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks (Paper ID: 133)'
author:
  name: Parth Patel
  email: f2016150@pilani.bits-pilani.ac.in
categories:
- CNN
- GANs
- Image Generation
tags:
- Hard
date: 2019-10-08 06:23:58 +0000

---
**Abstract:** Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256Ã—256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.

**Paper Link:** [https://arxiv.org/pdf/1612.03242.pdf](https://arxiv.org/pdf/1612.03242.pdf)

**Guidelines/Tasks:**

1. Implement the StackGAN architecture (i.e. Conditioning Augmentation model + Stage 1 generator and discriminator + Stage 2 generator and discriminator) as described in the paper using Keras (with Tensorflow backend) only. You may deviate slightly from the paper as far as no. of layers and dimensions of the layers are concerned (if you do not have enough compute power).
2. As mentioned in the paper, use CUB-200 birds images dataset for training purpose ([http://www.vision.caltech.edu/visipedia/CUB-200-2011.html](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)).
3. As shown in the paper, show some demo examples generated by your trained model.
4. Report the inception score (as defined in the paper) of your trained model. The only change to be made here is to use a lighter MobileNetV2 model for finetuning on CUB-200 image classification (rather than using Inception model as mentioned in the paper).
5. Note that you do not need to perform/implement the component analysis given in section 4.3 of the paper.

**Side Notes:** 
1. The paper itself is very short, very well explained (i.e. all minor details are mentioned) and not very difficult to understand. Only reason its placed in the Hard category is because it involves three separate training loops: Train stage-1 GAN, traing stage-2 GAN and finetune MobileNetV2 for StackGAN's performance evaluation.
2. For introduction to GANs, refer [https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09](https://medium.com/@jonathan_hui/gan-whats-generative-adversarial-networks-and-its-application-f39ed278ef09) .

**ID :** 133
